{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Relation Classification With R-BERT",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXm-1Gpqr_ME",
        "colab_type": "text"
      },
      "source": [
        "Trying to implement the architecture in this paper [Enriching Pre-trained Language Model with Entity Information for Relation Classification\n",
        "](https://arxiv.org/pdf/1905.08284.pdf)\n",
        "\n",
        "## Load Data\n",
        "\n",
        "We will use an well established data set for relationship classification to compare our results to the state-of-the-art on this dataset. Itâ€™s the task8 dataset from SemEval 2010. You can find it here: https://github.com/davidsbatista/Annotated-Semantic-Relationships-Datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5RUxbVIsNQ_",
        "colab_type": "code",
        "outputId": "c74f46b1-18e6-499b-a900-2f44ff78a693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "from urllib.request import urlretrieve\n",
        "import glob\n",
        "import tarfile\n",
        "\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "    \n",
        "# Download data\n",
        "url ='https://github.com/davidsbatista/Annotated-Semantic-Relationships-Datasets/raw/master/datasets/SemEval2010_task8_all_data.tar.gz'\n",
        "\n",
        "urlretrieve(url, 'data/SemEval2010_task8_all_data.tar.gz')\n",
        "\n",
        "\n",
        "tarf = tarfile.open(\"data/SemEval2010_task8_all_data.tar.gz\")\n",
        "tarf.extractall(path = 'data/')\n",
        "    \n",
        "\n",
        "glob.glob('data/*')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data/SemEval2010_task8_all_data', 'data/SemEval2010_task8_all_data.tar.gz']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKu3QPpYsUmn",
        "colab_type": "code",
        "outputId": "e7a56524-8314-42c4-a7b6-8dc2c1d4e9d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "with open(\"data/SemEval2010_task8_all_data/SemEval2010_task8_training/TRAIN_FILE.TXT\") as f:\n",
        "    train_file = f.readlines()\n",
        "    \n",
        "with open(\"data/SemEval2010_task8_all_data/SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT\") as f:\n",
        "    test_file = f.readlines()\n",
        "    \n",
        "test_file[:10]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['8001\\t\"The most common <e1>audits</e1> were about <e2>waste</e2> and recycling.\"\\n',\n",
              " 'Message-Topic(e1,e2)\\n',\n",
              " 'Comment: Assuming an audit = an audit document.\\n',\n",
              " '\\n',\n",
              " '8002\\t\"The <e1>company</e1> fabricates plastic <e2>chairs</e2>.\"\\n',\n",
              " 'Product-Producer(e2,e1)\\n',\n",
              " 'Comment: (a) is satisfied\\n',\n",
              " '\\n',\n",
              " '8003\\t\"The school <e1>master</e1> teaches the lesson with a <e2>stick</e2>.\"\\n',\n",
              " 'Instrument-Agency(e2,e1)\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6h_mUhksbQ0",
        "colab_type": "text"
      },
      "source": [
        "The training dataset consists of 8000 sentences with 10 different types of relations. Each sentence is annotated with a relation between two given nominals. The entities that are involved in this relations are identified by markers like <e1> in the text. For instance, the following sentence contains an example of the Entity-Destination relation between the\n",
        "nominals Flowers and chapel.\n",
        "\n",
        "`The system as described above has its greatest application in an arrayed <e1>configuration</e1> of antenna <e2>elements</e2>`\n",
        "\n",
        "Using this kind of special tokens is a quite useful way to tell the network that we want it to focus on to answer our question. The main advantage is that we can use a normal text classifier architecture to tackle the relationship extraction task. This approach can be used in many different ways.\n",
        "\n",
        "Now we need a function to parse the raw dataset into the format that is easy to use for model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR_DE_wZsXZu",
        "colab_type": "code",
        "outputId": "a10a61dc-46da-4544-8714-aaf4b01e36a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "def parse_dataset(raw):\n",
        "    sentences, relations = [], []\n",
        "    to_replace = [(\"\\\"\", \"\"), (\"\\n\", \"\"), (\"<\", \" <\"), (\">\", \"> \")]\n",
        "    last_was_sentence = False\n",
        "    for line in raw:\n",
        "        sl = line.split(\"\\t\")\n",
        "        if last_was_sentence:\n",
        "            relations.append(sl[0].split(\"(\")[0].replace(\"\\n\", \"\"))\n",
        "            last_was_sentence = False\n",
        "        if sl[0].isdigit():\n",
        "            sent = sl[1]\n",
        "            for rp in to_replace:\n",
        "                sent = sent.replace(rp[0], rp[1])\n",
        "            sentences.append(sent)\n",
        "            last_was_sentence = True\n",
        "    print(\"Found {} sentences\".format(len(sentences)))\n",
        "    return sentences, relations\n",
        "\n",
        "tr_sentences, tr_relations = parse_dataset(train_file)\n",
        "te_sentences, te_relations = parse_dataset(test_file)\n",
        "\n",
        "tr_sentences[0], tr_relations[0], te_sentences[0], te_relations[0]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8000 sentences\n",
            "Found 2717 sentences\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('The system as described above has its greatest application in an arrayed  <e1> configuration </e1>  of antenna  <e2> elements </e2> .',\n",
              " 'Component-Whole',\n",
              " 'The most common  <e1> audits </e1>  were about  <e2> waste </e2>  and recycling.',\n",
              " 'Message-Topic')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y35FQ3tsiXs",
        "colab_type": "code",
        "outputId": "4581b65b-ae11-4eea-d35e-45c233f594b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "n_relations = len(set(tr_relations))\n",
        "print(\"Found {} relations\\n\".format(n_relations))\n",
        "print(\"Relations:\\n{}\".format(list(set(tr_relations))))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10 relations\n",
            "\n",
            "Relations:\n",
            "['Member-Collection', 'Entity-Destination', 'Product-Producer', 'Other', 'Content-Container', 'Cause-Effect', 'Component-Whole', 'Instrument-Agency', 'Entity-Origin', 'Message-Topic']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bssctgcksste",
        "colab_type": "text"
      },
      "source": [
        "### Use BERT tokenizer to tokenize the input string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ6MZ56Jsplb",
        "colab_type": "code",
        "outputId": "571eb406-79c2-440e-993f-632fc2cde1d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!pip install bert-tensorflow\n",
        "!pip install tensorflow-hub\n",
        "import bert\n",
        "from bert import tokenization\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# This is a path to an uncased version of BERT\n",
        "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "\n",
        "addition_tokens = ['[EN1]', '[EN2]']\n",
        "\n",
        "\n",
        "class MyFullTokenizer(object):\n",
        "  \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
        "\n",
        "  def __init__(self, vocab_file, do_lower_case=True):\n",
        "    self.vocab = tokenization.load_vocab(vocab_file)\n",
        "    for i, atk in enumerate(addition_tokens):\n",
        "        self.vocab[atk] = self.vocab.pop('[unused{}]'.format(i))\n",
        "    #self.vocab = tokenization.load_vocab(vocab_file)\n",
        "    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
        "    self.basic_tokenizer = tokenization.BasicTokenizer(do_lower_case=do_lower_case)\n",
        "    self.wordpiece_tokenizer = tokenization.WordpieceTokenizer(vocab=self.vocab)\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    split_tokens = []\n",
        "    for token in self.basic_tokenizer.tokenize(text):\n",
        "      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "        split_tokens.append(sub_token)\n",
        "\n",
        "    return split_tokens\n",
        "\n",
        "  def convert_tokens_to_ids(self, tokens):\n",
        "    return tokenization.convert_by_vocab(self.vocab, tokens)\n",
        "\n",
        "  def convert_ids_to_tokens(self, ids):\n",
        "    return tokenization.convert_by_vocab(self.inv_vocab, ids)\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "    with tf.Graph().as_default():\n",
        "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
        "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "        with tf.Session() as sess:\n",
        "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
        "                                                tokenization_info[\"do_lower_case\"]])\n",
        "\n",
        "    return MyFullTokenizer(\n",
        "        vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.16.5)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (41.2.0)\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhCzl5Umsvir",
        "colab_type": "code",
        "outputId": "3520fad8-8584-481a-a1af-2f345b7317b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "# tokenize the sentences\n",
        "\n",
        "def tokenize(sentences):\n",
        "  bert_tks = []\n",
        "  for sentence in sentences:\n",
        "\n",
        "      sentence= sentence.replace('<e1>', '<e1>')\n",
        "      sentence= sentence.replace('</e1>', '<e1>')\n",
        "      sentence= sentence.replace('<e2>', '<e1>')\n",
        "      sentence= sentence.replace('</e2>', '<e1>')\n",
        "\n",
        "      trunks = sentence.split('<e1>')\n",
        "\n",
        "      bert_tokens = []\n",
        "      bert_tokens.append(\"[CLS]\")\n",
        "      if len(trunks) != 5:\n",
        "          raise ValueError('Something is wrong with this sentence: ' + sentence)\n",
        "\n",
        "      for i, trunk in enumerate(trunks):\n",
        "          tks = tokenizer.tokenize(trunk)\n",
        "          bert_tokens.extend(tks)\n",
        "          if i == 0 or i == 1:\n",
        "              bert_tokens.append('[EN1]')\n",
        "          elif i == 2 or i ==3:\n",
        "              bert_tokens.append('[EN2]')\n",
        "\n",
        "      bert_tokens.append(\"[SEP]\")\n",
        "\n",
        "      bert_tks.append(bert_tokens)\n",
        "  return bert_tks\n",
        "\n",
        "#tokenize([tr_sentences[0]])\n",
        "tr_bert_tks = tokenize(tr_sentences)\n",
        "te_bert_tks = tokenize(te_sentences)\n",
        "\n",
        "tr_bert_tks[0], te_bert_tks[0]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['[CLS]',\n",
              "  'the',\n",
              "  'system',\n",
              "  'as',\n",
              "  'described',\n",
              "  'above',\n",
              "  'has',\n",
              "  'its',\n",
              "  'greatest',\n",
              "  'application',\n",
              "  'in',\n",
              "  'an',\n",
              "  'array',\n",
              "  '##ed',\n",
              "  '[EN1]',\n",
              "  'configuration',\n",
              "  '[EN1]',\n",
              "  'of',\n",
              "  'antenna',\n",
              "  '[EN2]',\n",
              "  'elements',\n",
              "  '[EN2]',\n",
              "  '.',\n",
              "  '[SEP]'],\n",
              " ['[CLS]',\n",
              "  'the',\n",
              "  'most',\n",
              "  'common',\n",
              "  '[EN1]',\n",
              "  'audit',\n",
              "  '##s',\n",
              "  '[EN1]',\n",
              "  'were',\n",
              "  'about',\n",
              "  '[EN2]',\n",
              "  'waste',\n",
              "  '[EN2]',\n",
              "  'and',\n",
              "  'recycling',\n",
              "  '.',\n",
              "  '[SEP]'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeVF1xO3tKY3",
        "colab_type": "code",
        "outputId": "be734068-c667-4641-e1cd-af5193c561a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "# Padding the sequence so that they have the same length\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LEN = 128\n",
        "tr_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tr_bert_tks],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "te_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in te_bert_tks],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Construct masks for 2 entities\n",
        "def build_entity_mask(bert_tks):\n",
        "  marks = [[i for i, x in enumerate(tks) if x == \"[EN1]\" or x == '[EN2]'] for tks in bert_tks]\n",
        "\n",
        "  e1_masks = []\n",
        "  e2_masks = []\n",
        "  for i, mark in enumerate(marks):\n",
        "      e1_mask = np.zeros((MAX_LEN, ))\n",
        "      e2_mask = np.zeros((MAX_LEN, ))\n",
        "\n",
        "      e1_mask[mark[0] + 1: mark[1]] = 1.\n",
        "      e2_mask[mark[2] + 1: mark[3]] = 1.\n",
        "\n",
        "      e1_masks.append(e1_mask)\n",
        "      e2_masks.append(e2_mask)\n",
        "      \n",
        "  return e1_masks, e2_masks\n",
        "\n",
        "tr_e1masks, tr_e2masks = build_entity_mask(tr_bert_tks)\n",
        "te_e1masks, te_e2masks = build_entity_mask(te_bert_tks)\n",
        "\n",
        "tr_input_ids[0], tr_e1masks[0], tr_e2masks[0]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([  101,  1996,  2291,  2004,  2649,  2682,  2038,  2049,  4602,\n",
              "         4646,  1999,  2019,  9140,  2098,     1,  9563,     1,  1997,\n",
              "        13438,     2,  3787,     2,  1012,   102,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0]),\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUTfCWxVtQCm",
        "colab_type": "code",
        "outputId": "af778c5a-5864-4540-b86d-f96b3d32f067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# encode the label\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(tr_relations)\n",
        "\n",
        "tr_y = encoder.transform(tr_relations)\n",
        "te_y = encoder.transform(te_relations)\n",
        "tr_y[0], te_y[0]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DymSTAbEtT8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# attention mask for BERT model\n",
        "tr_attention_masks = [[float(i>0) for i in ii] for ii in tr_input_ids]\n",
        "te_attention_masks = [[float(i>0) for i in ii] for ii in te_input_ids]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw-XlAWbvMmQ",
        "colab_type": "text"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "### Wrap the BERT model into a tensorflow keras layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qu9nAqgtct8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "class BertLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_fine_tune_layers=10, **kwargs):\n",
        "        self.n_fine_tune_layers = n_fine_tune_layers\n",
        "        self.trainable = True\n",
        "        self.output_size = 768\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bert = hub.Module(\n",
        "            BERT_MODEL_HUB,\n",
        "            trainable=self.trainable,\n",
        "            name=\"{}_module\".format(self.name)\n",
        "        )\n",
        "        trainable_vars = self.bert.variables\n",
        "        \n",
        "        # Remove unused layers\n",
        "        trainable_vars = [var for var in trainable_vars if not (\"/cls/\" in var.name or 'pooler' in var.name)]\n",
        "        \n",
        "        # Select how many layers to fine tune\n",
        "        if self.n_fine_tune_layers == -1:\n",
        "            trainable_vars = []\n",
        "        else:\n",
        "            trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
        "        \n",
        "        # Add to trainable weights\n",
        "        for var in trainable_vars:\n",
        "            self._trainable_weights.append(var)\n",
        "        \n",
        "        # Add non-trainable weights\n",
        "        for var in self.bert.variables:\n",
        "            if var not in self._trainable_weights:\n",
        "                self._non_trainable_weights.append(var)\n",
        "        \n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        input_ids, input_mask, segment_ids = inputs\n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        \n",
        "        # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
        "        # Use \"sequence_outputs\" for token-level output.\n",
        "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)\n",
        "        return result['sequence_output']\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hHsSt1xwIVK",
        "colab_type": "text"
      },
      "source": [
        "The custom layer that average the entities vector and concat them like described in the paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENH3IyT3wHvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "from tensorflow import initializers\n",
        "from tensorflow.keras.layers import Layer, InputSpec, Dense, concatenate, Dropout\n",
        "\n",
        "class AverageAndConcat(Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.init = tf.initializers.random_uniform()\n",
        "        self.supports_masking = True\n",
        "        super(AverageAndConcat, self).__init__(** kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        bert_input_shape, e1mask_shape, e2mask_shape = input_shape\n",
        "        self.input_spec = [InputSpec(ndim=3)]\n",
        "        assert len(bert_input_shape) == 3\n",
        "\n",
        "        self.fc_0 = Dense(bert_input_shape[2])\n",
        "        self.fc_1 = Dense(bert_input_shape[2])\n",
        "        \n",
        "        #self._trainable_weights = [self.w]\n",
        "        super(AverageAndConcat, self).build(input_shape)\n",
        "        \n",
        "    def call(self, inputs, mask=None):\n",
        "        bert_input, e1_mask, e2_mask = inputs\n",
        "        h0 = K.tanh(bert_input[:, 0, :])\n",
        "        h1 = K.sum(bert_input * K.expand_dims(e1_mask, -1), axis = 1, keepdims=False)\n",
        "        h2 = K.sum(bert_input * K.expand_dims(e2_mask, -1), axis = 1, keepdims=False)\n",
        "\n",
        "        e1_len = K.sum(e1_mask, axis = 1, keepdims=True)\n",
        "        e2_len = K.sum(e2_mask, axis = 1, keepdims=True)\n",
        "\n",
        "        h1 = h1 / (e1_len + 1e-8)\n",
        "        h2 = h2 / (e2_len + 1e-8)\n",
        "\n",
        "        h1 = K.tanh(h1)\n",
        "        h2 = K.tanh(h2)\n",
        "        \n",
        "        # add dropout\n",
        "        h0 = Dropout(0.1)(h0)\n",
        "        h1 = Dropout(0.1)(h1)\n",
        "        h2 = Dropout(0.1)(h2)\n",
        "        \n",
        "        # fully connected layer for each vector\n",
        "        h0 = self.fc_0(h0)\n",
        "        h1 = self.fc_1(h1)\n",
        "        h2 = self.fc_1(h2)\n",
        "        \n",
        "        return concatenate([h0, h1, h2], -1)\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return self.compute_output_shape(input_shape)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        bert_input_shape, mark_input_shape = input_shape\n",
        "\n",
        "        return (bert_input_shape[0], bert_input_shape[2] * 3)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        if isinstance(input_mask, list):\n",
        "            return [None] * len(input_mask)\n",
        "        else:\n",
        "            return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmCkSj1Zvkzi",
        "colab_type": "text"
      },
      "source": [
        "### Now set up the main model here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR2LJ9lDvTNU",
        "colab_type": "code",
        "outputId": "5dfedadb-2e74-419e-a91c-7d360e9fbed9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Conv1D, Lambda, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import Bidirectional, concatenate, SpatialDropout1D, GlobalMaxPooling1D, add\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "\n",
        "in_id = Input(shape=(MAX_LEN,), name=\"input_ids\")\n",
        "in_mask = Input(shape=(MAX_LEN,), name=\"input_masks\")\n",
        "in_segment = Input(shape=(MAX_LEN,), name=\"segment_ids\")\n",
        "e1mask_in = Input(shape=(MAX_LEN,), name=\"e1_masks\")\n",
        "e2mask_in = Input(shape=(MAX_LEN,), name=\"e2_masks\")\n",
        "\n",
        "bert_inputs = [in_id, in_mask, in_segment]\n",
        "\n",
        "all_input = [in_id, in_mask, in_segment, e1mask_in, e2mask_in]\n",
        "\n",
        "\n",
        "# Instantiate the custom Bert Layer defined above\n",
        "bert_output = BertLayer(n_fine_tune_layers=0)(bert_inputs)\n",
        "\n",
        "concat_input = [bert_output, e1mask_in, e2mask_in]\n",
        "\n",
        "x = AverageAndConcat()(concat_input)\n",
        "x = Dropout(0.1)(x)\n",
        "out = Dense(units=n_relations, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(all_input, out)\n",
        "model.compile(optimizer=Adam(lr = 2e-5), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_layer (BertLayer)          (None, None, 768)    110104890   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "e1_masks (InputLayer)           [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "e2_masks (InputLayer)           [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "average_and_concat (AverageAndC (None, 2304)         1181184     bert_layer[0][0]                 \n",
            "                                                                 e1_masks[0][0]                   \n",
            "                                                                 e2_masks[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 2304)         0           average_and_concat[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           23050       dropout_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 111,309,124\n",
            "Trainable params: 110,095,882\n",
            "Non-trainable params: 1,213,242\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCZu1NWry0xl",
        "colab_type": "code",
        "outputId": "44dffa8b-2281-437c-b49b-5b16247b0dcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tr_y = np.expand_dims(tr_y, 2)\n",
        "te_y = np.expand_dims(te_y, 2)\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "    history = model.fit([tr_input_ids, np.array(tr_attention_masks), np.zeros_like(tr_input_ids), np.array(tr_e1masks), np.array(tr_e2masks)],\n",
        "                        tr_y,\n",
        "                        validation_data=([te_input_ids, np.array(te_attention_masks), np.zeros_like(te_input_ids), np.array(te_e1masks), np.array(te_e2masks)], te_y),\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=5,\n",
        "                        verbose=1)\n",
        "    \n",
        "    pred = model.predict([te_input_ids, np.array(te_attention_masks), np.zeros_like(te_input_ids), np.array(te_e1masks), np.array(te_e2masks)])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2717 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "8000/8000 [==============================] - 468s 58ms/sample - loss: 0.9035 - acc: 0.6880 - val_loss: 0.5040 - val_acc: 0.8145\n",
            "Epoch 2/5\n",
            "8000/8000 [==============================] - 458s 57ms/sample - loss: 0.2296 - acc: 0.9289 - val_loss: 0.4788 - val_acc: 0.8325\n",
            "Epoch 3/5\n",
            "8000/8000 [==============================] - 461s 58ms/sample - loss: 0.0476 - acc: 0.9874 - val_loss: 0.5061 - val_acc: 0.8469\n",
            "Epoch 4/5\n",
            "8000/8000 [==============================] - 459s 57ms/sample - loss: 0.0106 - acc: 0.9984 - val_loss: 0.5692 - val_acc: 0.8414\n",
            "Epoch 5/5\n",
            "8000/8000 [==============================] - 460s 58ms/sample - loss: 0.0051 - acc: 0.9991 - val_loss: 0.6062 - val_acc: 0.8469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NTHwi87fRzt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b2a085f-d154-40ca-840b-e767839be5cd"
      },
      "source": [
        "# time for evaluation\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "pred_cl = np.argmax(pred, -1)\n",
        "precision, recall, fscore, support = score(te_y, pred_cl)\n",
        "\n",
        "# remove 'Other'\n",
        "score = [s for i, s in enumerate(fscore) if i != encoder.transform(['Other'])[0]]\n",
        "np.mean(score)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.881904965271868"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDIjw55QqbrI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}